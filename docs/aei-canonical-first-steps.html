<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AEI Canonical First Steps | Global Movement Research</title>
    <link rel="stylesheet" href="../css/style.css">
    <style>
        .doc-header {
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 2px solid var(--earth);
        }
        
        .doc-header h1 {
            font-family: var(--font-display);
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }
        
        .doc-subtitle {
            font-size: 1.125rem;
            color: var(--ocean);
            margin-bottom: 1rem;
        }
        
        .doc-meta {
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .doc-section {
            margin-bottom: 3rem;
        }
        
        .doc-section h2 {
            font-family: var(--font-display);
            font-size: 1.75rem;
            color: var(--text-primary);
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border);
        }
        
        .doc-section h3 {
            font-family: var(--font-display);
            font-size: 1.25rem;
            color: var(--text-primary);
            margin-top: 2rem;
            margin-bottom: 0.75rem;
        }
        
        .doc-section h4 {
            font-family: var(--font-body);
            font-size: 1rem;
            font-weight: 600;
            color: var(--earth);
            margin-top: 1.5rem;
            margin-bottom: 0.5rem;
        }
        
        .doc-section p {
            color: var(--text-secondary);
            line-height: 1.7;
            margin-bottom: 1rem;
            max-width: 800px;
        }
        
        .doc-section ul, .doc-section ol {
            margin: 1rem 0;
            padding-left: 1.5rem;
            max-width: 800px;
        }
        
        .doc-section li {
            margin-bottom: 0.5rem;
            color: var(--text-secondary);
            line-height: 1.6;
        }
        
        .doc-section li strong {
            color: var(--text-primary);
        }
        
        /* Phase cards */
        .phase-card {
            background: var(--surface);
            border-radius: 8px;
            padding: 1.5rem 2rem;
            margin: 1.5rem 0;
            border-left: 4px solid var(--earth);
        }
        
        .phase-card h3 {
            margin-top: 0;
            color: var(--earth);
        }
        
        .phase-card:nth-of-type(2) {
            border-left-color: var(--ocean);
        }
        
        .phase-card:nth-of-type(2) h3 {
            color: var(--ocean);
        }
        
        .phase-card:nth-of-type(3) {
            border-left-color: var(--terracotta);
        }
        
        .phase-card:nth-of-type(3) h3 {
            color: var(--terracotta);
        }
        
        .phase-card:nth-of-type(4) {
            border-left-color: #6b5b95;
        }
        
        .phase-card:nth-of-type(4) h3 {
            color: #6b5b95;
        }
        
        /* Pipeline steps */
        .pipeline-steps {
            counter-reset: pipeline;
            list-style: none;
            padding: 0;
        }
        
        .pipeline-steps li {
            counter-increment: pipeline;
            padding: 1rem 1rem 1rem 3.5rem;
            position: relative;
            background: var(--surface);
            margin-bottom: 0.5rem;
            border-radius: 4px;
        }
        
        .pipeline-steps li::before {
            content: counter(pipeline);
            position: absolute;
            left: 1rem;
            top: 1rem;
            width: 1.75rem;
            height: 1.75rem;
            background: var(--ocean);
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.85rem;
            font-weight: 600;
        }
        
        /* Interface section */
        .interface-block {
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            border-radius: 8px;
            padding: 2rem;
            margin: 1.5rem 0;
            color: white;
        }
        
        .interface-block h4 {
            color: #0ff;
            font-family: var(--font-mono);
            font-size: 0.85rem;
            text-transform: uppercase;
            letter-spacing: 0.1em;
            margin-top: 0;
        }
        
        .interface-block p {
            color: rgba(255,255,255,0.85);
        }
        
        .interface-block ul {
            color: rgba(255,255,255,0.85);
        }
        
        .interface-block li {
            color: rgba(255,255,255,0.85);
        }
        
        /* Code/technical blocks */
        .tech-note {
            background: #f8f5f0;
            border-left: 3px solid var(--earth);
            padding: 1rem 1.5rem;
            margin: 1rem 0;
            font-size: 0.9rem;
        }
        
        .tech-note code {
            font-family: var(--font-mono);
            background: rgba(0,0,0,0.05);
            padding: 0.1rem 0.3rem;
            border-radius: 3px;
        }
        
        /* Checklist */
        .checklist {
            list-style: none;
            padding: 0;
        }
        
        .checklist li {
            padding: 0.5rem 0 0.5rem 2rem;
            position: relative;
        }
        
        .checklist li::before {
            content: '☐';
            position: absolute;
            left: 0;
            color: var(--earth);
        }
        
        /* Back link */
        .back-link {
            display: inline-block;
            margin-bottom: 2rem;
            color: var(--ocean);
            text-decoration: none;
            font-size: 0.9rem;
        }
        
        .back-link:hover {
            text-decoration: underline;
        }
        
        /* References */
        .references-list {
            list-style: none;
            padding: 0;
        }
        
        .references-list li {
            margin-bottom: 1rem;
            padding-left: 1.5rem;
            position: relative;
            font-size: 0.95rem;
        }
        
        .references-list li::before {
            content: '•';
            position: absolute;
            left: 0;
            color: var(--earth);
        }
        
        @media (max-width: 768px) {
            .doc-header h1 {
                font-size: 2rem;
            }
            
            .phase-card {
                padding: 1rem 1.5rem;
            }
            
            .interface-block {
                padding: 1.5rem;
            }
        }
    </style>
</head>
<body>
    <nav class="nav">
        <div class="nav-container">
            <a href="../index.html" class="nav-logo">GMR</a>
            <ul class="nav-links">
                <li><a href="../maps.html">Map</a></li>
                <li><a href="../dances.html">Dances</a></li>
                <li><a href="../compare.html">Compare</a></li>
                <li><a href="../methods.html">Methods</a></li>
            </ul>
        </div>
    </nav>

    <main class="main-content">
        <div class="container">
            <a href="../methods.html" class="back-link">← Back to Methods</a>
            
            <header class="doc-header">
                <h1>AEI Canonical First Steps</h1>
                <p class="doc-subtitle">Technical Documentation</p>
                <p class="doc-meta">Global Movement Research · Version 1.0</p>
            </header>

            <!-- Overview -->
            <section class="doc-section">
                <h2>Overview</h2>
                <p>This document describes the technical pipeline for Artificial Embodied Intelligence (AEI) within Global Movement Research. AEI is designed to activate archived movement—transforming recordings into analyzable, comparable, and queryable data—without generating new choreography or overriding community knowledge.</p>
                
                <p>The pipeline moves through four phases: Data Ingestion, Feature Extraction, Relationship Discovery, and Interface Design. Each phase builds on the last, gradually converting raw recordings into an archive that can think with researchers rather than for them.</p>
            </section>

            <!-- Implementation Phases -->
            <section class="doc-section">
                <h2>Implementation Phases</h2>
                
                <div class="phase-card">
                    <h3>Phase 1: Data Ingestion</h3>
                    <p>In the first phase, AEI systems gather movement material from multiple sources and normalize it into a consistent internal format.</p>
                    
                    <p><strong>Typical inputs include:</strong></p>
                    <ul>
                        <li>Video recordings from rehearsals, performances, or fieldwork</li>
                        <li>Motion capture sessions from GMR labs or partner studios</li>
                        <li>Existing digital archives containing skeletal data or FBX files</li>
                        <li>Associated documents—lab notes, cue sheets, StoryMap links</li>
                    </ul>
                    
                    <p><strong>The ingestion process:</strong></p>
                    <ol>
                        <li>Register each recording as a <code>movement_record</code> with a unique identifier.</li>
                        <li>Attach core metadata—performer, tradition, location, capture date, and consent conditions.</li>
                        <li>Convert raw files to stable archival formats (standardized video codecs, FBX or CSV for skeletal data).</li>
                        <li>Store results in an AEI-compliant repository where they can be queried, versioned, and linked.</li>
                    </ol>
                    
                    <p>Phase 1 ends when every movement sample is represented as a machine-readable record with clear metadata and traceable provenance.</p>
                </div>
                
                <div class="phase-card">
                    <h3>Phase 2: Feature Extraction</h3>
                    <p>In Phase 2, raw trajectories become structured features that AEI systems can analyze and compare.</p>
                    
                    <p><strong>Key operations include:</strong></p>
                    <ul>
                        <li><strong>Skeleton extraction</strong> — Identifying joint positions over time, either from motion capture or from computer vision models applied to video.</li>
                        <li><strong>Relational encoding</strong> — Converting joint trajectories into qualitative relations using QTC (approach/separate, left/right, up/down).</li>
                        <li><strong>Derived features</strong> — Computing higher-level descriptors such as tempo, movement density, use of levels, symmetry, or directional bias.</li>
                        <li><strong>Event markers</strong> — Tagging important instants such as weight shifts, turns, jumps, or changes in facing.</li>
                    </ul>
                    
                    <p>The output of Phase 2 is a set of feature streams aligned with the original recording. These features become the vocabulary that AEI uses to talk about movement.</p>
                </div>
                
                <div class="phase-card">
                    <h3>Phase 3: Relationship Discovery</h3>
                    <p>Phase 3 focuses on patterns, connections, and anomalies within the growing archive.</p>
                    
                    <p><strong>Typical operations:</strong></p>
                    <ul>
                        <li><strong>Sequence alignment</strong> — Using SAMs to compare QTC sequences within and across traditions, identifying shared phrases and motifs.</li>
                        <li><strong>Similarity graphs</strong> — Building networks that connect recordings with similar movement signatures, rhythmic structures, or spatial trajectories.</li>
                        <li><strong>Clustering</strong> — Grouping dances into families based on structural likeness, then checking these families against historical and ethnographic knowledge.</li>
                        <li><strong>Diffusion maps</strong> — Linking relationship patterns to geography through ACF—how a motif appears across confluence zones.</li>
                    </ul>
                    
                    <p>Relationship discovery does not replace interpretation. It surfaces candidates for deeper study and creates a set of hypotheses that human researchers can test with communities and archival evidence.</p>
                </div>
                
                <div class="phase-card">
                    <h3>Phase 4: Interface Design</h3>
                    <p>In Phase 4, the archive is turned outward so that humans can work with it.</p>
                    
                    <p><strong>Interface layers may include:</strong></p>
                    <ul>
                        <li><strong>Search and query tools</strong> — Letting users filter by geography, tradition, movement qualities, or similarity to a reference dance.</li>
                        <li><strong>Visualization panels</strong> — Showing skeleton replays, QTC timelines, similarity matrices, and maps that connect movement families to places.</li>
                        <li><strong>Study views</strong> — Combining playback, annotations, and notes so that students, dancers, and researchers can analyze a recording in context.</li>
                        <li><strong>Export tools</strong> — Allowing selected data, screenshots, or visualizations to be exported into teaching materials, StoryMaps, or publications.</li>
                    </ul>
                    
                    <p>Interface design is where AEI's technical work becomes legible. The goal is not to hide complexity, but to organize it so that movement experts can explore, question, and challenge what the system presents.</p>
                </div>
            </section>

            <!-- QTC Encoding -->
            <section class="doc-section">
                <h2>QTC Encoding Pipeline</h2>
                
                <p>QTC encoding converts raw trajectories into a qualitative description of relative motion. For AEI, this process links numerical data to structured movement language.</p>
                
                <ol class="pipeline-steps">
                    <li>
                        <strong>Select reference pairs</strong><br>
                        Define which body points or objects will be related. Examples: left hand to torso, feet to each other, dancer to a fixed point in space.
                    </li>
                    <li>
                        <strong>Track positions</strong><br>
                        For each frame, record the three-dimensional coordinates of each point—either from motion capture or from a pose estimation system.
                    </li>
                    <li>
                        <strong>Compute relative motion</strong><br>
                        For every time step, compute relative displacement and velocity between paired points (closer/further, left/right, up/down).
                    </li>
                    <li>
                        <strong>Assign qualitative symbols</strong><br>
                        Translate numeric values into QTC symbols. Common convention: <code>-1</code> for movement in one direction, <code>0</code> for no significant change, <code>+1</code> for movement in the opposite direction.
                    </li>
                    <li>
                        <strong>Build QTC sequences</strong><br>
                        Concatenate symbols over time to create QTC sequences or matrices. Each sequence summarizes how the relationship between joints evolves throughout the dance.
                    </li>
                    <li>
                        <strong>Prepare for analysis</strong><br>
                        Store QTC sequences alongside original trajectories and metadata. These sequences feed into SAM-based alignment, clustering, and pattern discovery.
                    </li>
                </ol>
                
                <div class="tech-note">
                    <strong>Note:</strong> QTC does not replace watching the dance. It creates an additional view that preserves relational structure in a compact form, making large-scale comparison computationally possible.
                </div>
            </section>

            <!-- AEI Interface -->
            <section class="doc-section">
                <h2>AEI Interface</h2>
                <p class="doc-subtitle" style="margin-top: -0.5rem;">Prototype Console</p>
                
                <p>The AEI Interface is a prototype console for exploring movement records inside the GMR archive. It exposes the core operations described above so that researchers and dancers can see how data ingestion, feature extraction, and relationship discovery work in practice.</p>
                
                <div class="interface-block">
                    <h4>1. Choose Dataset</h4>
                    <p>Select a dataset to work with. Options may include:</p>
                    <ul>
                        <li>Individual recordings (e.g., "Ekombi fieldwork 2019, Calabar")</li>
                        <li>Lab sessions (e.g., "GMR Lab, Bata mocap series")</li>
                        <li>Curated sets (e.g., "Niger Delta coastal dances" or "Kandyan solo repertoire")</li>
                    </ul>
                    <p>Each dataset summary shows: number of recordings and performers, capture methods used, consent level and permitted uses. Users must confirm that the intended operation stays within permitted uses before continuing.</p>
                </div>
                
                <div class="interface-block">
                    <h4>2. Pick Operation</h4>
                    <p>Choose what you want AEI to do with the selected dataset:</p>
                    <ul>
                        <li><strong>QTC encoding</strong> — Generate or inspect QTC sequences for the recordings.</li>
                        <li><strong>Similarity search</strong> — Find dances that are structurally similar to a reference recording.</li>
                        <li><strong>Motif discovery</strong> — Identify recurring movement phrases across the dataset using SAMs.</li>
                        <li><strong>Diffusion view</strong> — Map discovered motifs to confluence zones and geographic regions.</li>
                    </ul>
                    <p>Each operation shows which features it will use and estimated compute cost. Results are always accompanied by a reminder that they are hypotheses, not conclusions.</p>
                </div>
                
                <div class="interface-block">
                    <h4>3. Inspect Results</h4>
                    <p>The results viewer combines multiple views:</p>
                    <ul>
                        <li>Synchronized video or skeleton replay</li>
                        <li>QTC timelines or matrices</li>
                        <li>Similarity scores and alignment maps</li>
                        <li>Optional geographic overlays (ACF diffusion maps)</li>
                    </ul>
                    <p>Users can scrub through time, compare recordings side by side, click highlighted segments to see SAM alignments, and jump from a motif to other recordings where it appears. Annotations created here can be exported for teaching, writing, or further lab work.</p>
                </div>
                
                <div class="interface-block">
                    <h4>4. Context and Ethics</h4>
                    <p>Every result is shown alongside core metadata including tradition, location, capture context, consent and permitted uses, attribution requirements, and community contacts.</p>
                    <p><strong>User checklist:</strong></p>
                    <ul class="checklist">
                        <li>Respect the stated consent level</li>
                        <li>Name performers and communities in any public output</li>
                        <li>Consult source communities when working with sensitive material or surprising findings</li>
                        <li>Treat all AEI findings as prompts for further inquiry rather than as final truth</li>
                    </ul>
                </div>
                
                <p>This interface is designed as a working sketch. Functionality will grow as new labs, datasets, and collaborations come online, but the structure—consent-first, movement-centered, and hypothesis-driven—remains the same.</p>
            </section>

            <!-- References -->
            <section class="doc-section">
                <h2>References</h2>
                
                <ul class="references-list">
                    <li>Van de Weghe, Nico. "Representing and Reasoning about Moving Objects: A Qualitative Approach." PhD dissertation, Ghent University, 2004. Foundational work on Qualitative Trajectory Calculus.</li>
                    <li>Chavoshi, Sajad H., et al. "Exploring Dance Movement Data Using Sequence Alignment Methods." <em>PLOS ONE</em> 10, no. 6, 2015.</li>
                    <li>Raheb, Katerina, and Yannis Ioannidis. "Modeling and Querying Dance Movement in Digital Libraries."</li>
                    <li>Wallace, Naomi, et al. "Embodying an Interactive AI for Dance." 2023.</li>
                    <li>Caswell, Michelle. <em>Urgent Archives: Enacting Liberatory Memory Work</em>. Routledge, 2021.</li>
                    <li>Taylor, Diana. <em>The Archive and the Repertoire: Performing Cultural Memory in the Americas</em>. Duke University Press, 2003.</li>
                </ul>
            </section>
        </div>
    </main>

    <footer class="footer">
        <div class="container">
            <div class="footer-grid">
                <div class="footer-section">
                    <h4>Global Movement Research</h4>
                    <p>Documenting human movement heritage through ethical motion capture and open archives.</p>
                </div>
                <div class="footer-section">
                    <h4>Archive</h4>
                    <ul>
                        <li><a href="../maps.html">Confluence Map</a></li>
                        <li><a href="../dances.html">Dance Visualizations</a></li>
                        <li><a href="../compare.html">Comparison Tool</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h4>Resources</h4>
                    <ul>
                        <li><a href="../methods.html">Methods</a></li>
                        <li><a href="aei-canonical-first-steps.html">AEI Documentation</a></li>
                        <li><a href="#">GitHub Repository</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>Global Movement Research</p>
                <p>Sinclair Emoghene, Researcher</p>
                <p>University of Texas at Austin | Sinclair Dance | StepMate</p>
            </div>
        </div>
    </footer>

    <script>
        // Nav scroll effect
        const nav = document.querySelector('.nav');
        window.addEventListener('scroll', () => {
            if (window.scrollY > 50) {
                nav.classList.add('scrolled');
            } else {
                nav.classList.remove('scrolled');
            }
        });
    </script>
</body>
</html>
